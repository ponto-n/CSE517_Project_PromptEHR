{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version 1.13.1+cu116\n",
      "Found GPU: NVIDIA GeForce RTX 3060, with 12.8843776 GB of memory\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'Using torch version {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    device_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    device_memory /= 1e9\n",
    "    print(f'Found GPU: {torch.cuda.get_device_name()}, with {device_memory} GB of memory')\n",
    "\n",
    "batch_size = int(device_memory/3)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\pytrial\\data\\patient_data.py:95: UserWarning: No metadata provided. Metadata will be automatically detected from your data. This process may not be accurate. We recommend writing metadata to ensure correct data handling.\n",
      "  warnings.warn('No metadata provided. Metadata will be automatically '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "# from promptehr import load_demo_data\n",
    "\n",
    "# load pytrial demodata, supported by PyTrial package to load the demo EHR data\n",
    "from pytrial.data.demo_data import load_mimic_ehr_sequence\n",
    "from pytrial.tasks.trial_simulation.data import SequencePatient\n",
    "\n",
    "# see the input format\n",
    "data_dir = 'C:\\\\Users\\\\noah\\\\cse517_project_code\\\\data_conversion\\\\datasets\\\\train'\n",
    "# val_dir = 'C:\\\\Users\\\\noah\\\\cse517_project_code\\\\data_conversion\\\\datasets\\\\val'\n",
    "demo = load_mimic_ehr_sequence(input_dir=data_dir, n_sample=15000)\n",
    "# val_data = load_mimic_ehr_sequence(input_dir=val_dir)\n",
    "\n",
    "# build sequence dataset\n",
    "seqdata = SequencePatient(data={'v':demo['visit'], 'y':demo['mortality'], 'x':demo['feature'],},\n",
    "    metadata={\n",
    "        'visit':{'mode':'dense'},\n",
    "        'label':{'mode':'tensor'}, \n",
    "        'voc':demo['voc'],\n",
    "        'max_visit':20,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# val_seqdata = SequencePatient(data={'v':val_data['visit'], 'y':val_data['mortality'], 'x':val_data['feature'],},\n",
    "#     metadata={\n",
    "#         'visit':{'mode':'dense'},\n",
    "#         'label':{'mode':'tensor'}, \n",
    "#         'voc':val_data['voc'],\n",
    "#         'max_visit':20,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "print('visit', demo['visit'][0]) # a list of visit events\n",
    "print('mortality', demo['mortality'][0]) # array of labels\n",
    "print('feature', demo['feature'][0]) # array of patient baseline features\n",
    "print('voc', demo['voc']) # dict of dicts containing the mapping from index to the original event names\n",
    "print('order', demo['order']) # a list of three types of code\n",
    "print('n_num_feature', demo['n_num_feature']) # int: a number of patient's numerical features\n",
    "print('cat_cardinalities', demo['cat_cardinalities']) # list: a list of cardinalities of patient's categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'DataTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from promptehr import PromptEHR\n",
    "\n",
    "# fit the model\n",
    "model = PromptEHR(\n",
    "    code_type=demo['order'],\n",
    "    n_num_feature=demo['n_num_feature'],\n",
    "    cat_cardinalities=demo['cat_cardinalities'],\n",
    "    num_worker=0,\n",
    "    eval_step=20000,\n",
    "    epoch=1,\n",
    "    batch_size=4,\n",
    "    # device=[1,2],\n",
    "    output_dir='C:\\\\Users\\\\noah\\\\cse517_project_code\\\\promptEHR_logs'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n",
      "  Number of trainable parameters = 145501440\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003000497817993164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3750,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebc3e62235e4f83a34b4bceb83354b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2681, 'learning_rate': 3.900709219858156e-05, 'epoch': 0.27}\n",
      "evaluation for code diag.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0020003318786621094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 938,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8a7088151b415eb26056f1747760d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     train_data\u001b[39m=\u001b[39;49mseqdata,\n\u001b[0;32m      3\u001b[0m     val_data\u001b[39m=\u001b[39;49mseqdata,\n\u001b[0;32m      4\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\promptehr\\promptehr.py:178\u001b[0m, in \u001b[0;36mPromptEHR.fit\u001b[1;34m(self, train_data, val_data)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_model()\n\u001b[0;32m    177\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(train_data\u001b[39m=\u001b[39;49mtrain_data,val_data\u001b[39m=\u001b[39;49mval_data)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\promptehr\\promptehr.py:492\u001b[0m, in \u001b[0;36mPromptEHR._fit\u001b[1;34m(self, train_data, val_data)\u001b[0m\n\u001b[0;32m    479\u001b[0m mimic_val_collator \u001b[39m=\u001b[39m MimicDataCollator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_tokenizer, \n\u001b[0;32m    480\u001b[0m     code_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mcode_type\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    481\u001b[0m     n_num_feature\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mn_num_feature\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    482\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    484\u001b[0m trainer \u001b[39m=\u001b[39m PromptEHRTrainer(\n\u001b[0;32m    485\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m    486\u001b[0m     args\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     val_data_collator\u001b[39m=\u001b[39mmimic_val_collator,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer.py:1826\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1823\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1826\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer.py:2089\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2083\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m   2084\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[0;32m   2085\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2086\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2087\u001b[0m             )\n\u001b[0;32m   2088\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2089\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\promptehr\\trainer.py:114\u001b[0m, in \u001b[0;36mPromptEHRTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m code_type \u001b[39min\u001b[39;00m code_type_list:\n\u001b[0;32m    113\u001b[0m     eval_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_eval_dataloader(eval_dataset, code_type\u001b[39m=\u001b[39mcode_type) \u001b[39m# change the data collator\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m    115\u001b[0m         eval_dataloader,\n\u001b[0;32m    116\u001b[0m         description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    117\u001b[0m         \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m    118\u001b[0m         \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m         prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    120\u001b[0m         ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m    121\u001b[0m         metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m     output_metrics[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_ppl_\u001b[39m\u001b[39m{\u001b[39;00mcode_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmetrics[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    125\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(output_metrics)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\promptehr\\trainer.py:245\u001b[0m, in \u001b[0;36mPromptEHRTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     logits \u001b[39m=\u001b[39m nested_numpify(preds_host)\n\u001b[1;32m--> 245\u001b[0m     all_preds \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(all_preds, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     inputs_decode \u001b[39m=\u001b[39m nested_numpify(inputs_host)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[0;32m    110\u001b[0m     new_tensors\n\u001b[0;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;49;00m t, n \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(tensors, new_tensors))\n\u001b[0;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[0;32m    110\u001b[0m     new_tensors\n\u001b[0;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer_pt_utils.py:121\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[0;32m    118\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[0;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type for concatenation: got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\noah\\miniconda3\\envs\\cse517\\lib\\site-packages\\transformers\\trainer_pt_utils.py:99\u001b[0m, in \u001b[0;36mnumpy_pad_and_concatenate\u001b[1;34m(array1, array2, padding_index)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m     98\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull_like(array1, padding_index, shape\u001b[39m=\u001b[39mnew_shape)\n\u001b[1;32m---> 99\u001b[0m result[: array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], : array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m array1\n\u001b[0;32m    100\u001b[0m result[array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] :, : array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m array2\n\u001b[0;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data=seqdata,\n",
    "    val_data=seqdata,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
